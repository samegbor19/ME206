{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samegbor19/ME206/blob/main/OCRL_106B_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANZSjoVF4G5D"
      },
      "source": [
        "# ME/EECS/BioE C106B/206B Homework 6\n",
        "\n",
        "\n",
        "## Optimal Control and Reinforcement Learning\n",
        "In this notebook, you'll find the coding section for homework 6, Optimal Control and Reinforcement Learning. To get started, **make a copy** of this notebook and save it in your drive.\n",
        "\n",
        "If you haven't used a colab notebook environment before, notebooks are just like normal Python files! Each section of the code is split up into a small block called a code cell, which you can run by pressing shift+enter or the play button on the left side of each cell. Run the following code cell to import the libraries we'll be using in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zCV4eiYn8nYU"
      },
      "outputs": [],
      "source": [
        "#import librarires\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyzIGsLo5BSI"
      },
      "source": [
        "## Problem 1: The Linear Quadratic Regulator\n",
        "In this question, we'll implement a finite horizon linear quadratic regulator (LQR) controller to stabilize a discrete time system about the origin.\n",
        "Let's review the formulation of the LQR problem. We have a linear discrete time system:\n",
        "$$\n",
        "x_{k+1} = Ax_k + Bu_k\n",
        "$$\n",
        "And would like to find the optimal sequence of control inputs $u_0, ..., u_{N-1}$ that minimize the cost function:\n",
        "$$\n",
        "J =  x_N^T Q_f x_N + \\sum_{k = 0}^{N-1}(x_k^T Q x_k + u_k^T R u_k)\n",
        "$$\n",
        "We can show that the optimal inputs $u_i$ in this sequence may be calculated through the expression:\n",
        "$$\n",
        "u_i = -(R + B^TP_{i+1}B)^{-1}B^TP_{i+1}A x_i\n",
        "$$\n",
        "Where $P_i$ is a special matrix that comes from a recursive equation known as a Riccati equation:\n",
        "$$\n",
        "P_i = A^TP_{i+1}A + Q - A^TP_{i+1}B(R + B^TP_{i+1}B)^{-1}B^TP_{i+1}A\n",
        "$$\n",
        "To solve for the control input, we must calculate the $P_i$ matrix by iterating backwards in time from $P_N = Q_f$, the matrix in the terminal cost. Let's implement the different pieces of this controller!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeTtT_Ud8_LK"
      },
      "source": [
        "#### Riccati Equation\n",
        "First, let's write a function that implements the recursive Riccati equation from above. It should take in the matrices from the cost function, system dynamics, and $P_{i+1}$, and return $P_i$.\n",
        "\n",
        "This won't be the only function we'll need, however! Our first function only gives us a way to calculate $P_i$ from $P_{i+1}$. We'll need a second function that computed $P_i$ from the final condition $P_N = Q_f$. This function should perform the following process:\n",
        "\n",
        "1.   Start with $P_N = Q_f$\n",
        "2.   Step $P_N$ backwards in time using the recursive function to get $P_{N-1}$\n",
        "3. Continue stepping the $P$ matrix backwards until you have arrived at the correct index, $i$\n",
        "4. Return $P_i$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uHY9edkA4iUE"
      },
      "outputs": [],
      "source": [
        "def eval_recursion_step(A, B, Q, R, Pip1):\n",
        "  \"\"\"\n",
        "  Function to recursively evaluate Pi from Pi+1 in the riccati equation.\n",
        "  Inputs:\n",
        "    A (NumPy Array): The A matrix from the system dynamics\n",
        "    B (NumPy Array): The B matrix from the system dynamics\n",
        "    Q (NumPy Array): The Q weight matrix in the LQR cost function\n",
        "    R (NumPy Array): The R weight matrix in the LQR cost function\n",
        "    Pip1 (NumPy Array): Pi+1, the P matrix one step ahead in time\n",
        "  Returns:\n",
        "    Pi (NumPy Array): The matrix Pi, as calculated from the recursive Riccati equation\n",
        "  \"\"\"\n",
        "  #Return the recursive Pi value\n",
        "  Pi = ...\n",
        "  return Pi\n",
        "\n",
        "def eval_p_i(A, B, Q, R, PN, N, i):\n",
        "  \"\"\"\n",
        "  Function to recursively evaluate Pi from PN in the riccati equation.\n",
        "  Inputs:\n",
        "    A (NumPy Array): The A matrix from the system dynamics\n",
        "    B (NumPy Array): The B matrix from the system dynamics\n",
        "    Q (NumPy Array): The Q weight matrix in the LQR cost function\n",
        "    R (NumPy Array): The R weight matrix in the LQR cost function\n",
        "    PN (NumPy Array): P(N) = Qf, the P matrix in the terminal cost\n",
        "    N (int): The horizon, N, in the cost function \n",
        "    i (int): The index of Pi we'd like to calculate (zero-indexed)\n",
        "  Returns:\n",
        "    Pi (NumPy Array): The matrix Pi, as calculated from iterating the Riccati equation\n",
        "\n",
        "  Hint:\n",
        "    To find the number of times we need to iterate, see how close i is to the \n",
        "    total number of steps, N. Try using a loop to iterate Pi backwards in time.\n",
        "  \"\"\"\n",
        "  #initialize the value of Pi\n",
        "  Pi = ...\n",
        "  return Pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0AnE2gb9Use"
      },
      "source": [
        "#### LQR Controller\n",
        "Now, let's implement a function that returns the control input $u_i$ from the LQR solution discussed above. Recall, the input is:\n",
        "$$\n",
        "u_i = -(R + B^TP_{i+1}B)^{-1}B^TP_{i+1}A x_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQJWnMoa9aM_"
      },
      "outputs": [],
      "source": [
        "def eval_lqr_input(A, B, Q, R, Pip1, xi):\n",
        "  \"\"\"\n",
        "  Function to evaluate the LQR control input, ui.\n",
        "  Inputs:\n",
        "    A (NumPy Array): The A matrix from the system dynamics\n",
        "    B (NumPy Array): The B matrix from the system dynamics\n",
        "    Q (NumPy Array): The Q weight matrix in the LQR cost function\n",
        "    R (NumPy Array): The R weight matrix in the LQR cost function\n",
        "    Pip1 (NumPy Array): Pi+1, the P matrix one step ahead in time\n",
        "    xi (NumPy Array): The state vector of the system at time step i\n",
        "  Returns:\n",
        "    ui (NumPy Array): The LQR input to the system at time step i\n",
        "  \"\"\"\n",
        "  #evaluate and return the control input ui\n",
        "  ui = ...\n",
        "  return ui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK0Coipe-Cfx"
      },
      "source": [
        "#### LQR Simulation\n",
        "Let's simulate the LQR controller using a simple system. Run the code cell below to test out your LQR controller. If your contoller works, you should see the trajectories of the system converging smoothly to the origin. Attach these plots to your solution!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJTxt8Kx990B"
      },
      "outputs": [],
      "source": [
        "# Define system matrices\n",
        "A = np.array([[0, 1, 0], [0, 0, 1], [1, -2, 4]])\n",
        "B = np.array([[0, 0, 1]]).T\n",
        "\n",
        "# Define initial condition\n",
        "x0 = np.array([[1, 1, 1]]).T\n",
        "\n",
        "# Define LQR weight matrices (positive definite or positive semidefinite)\n",
        "Qf = np.eye(3)\n",
        "P = np.eye(3)\n",
        "R = np.eye(1)\n",
        "Q = np.eye(3)\n",
        "\n",
        "# Define LQR horizon\n",
        "N = 20\n",
        "\n",
        "#initialize the state at the initial condition\n",
        "xi = x0\n",
        "\n",
        "#define history vector\n",
        "xHist = x0\n",
        "\n",
        "#simulate the system over the control horizon\n",
        "for i in range(N):\n",
        "  #find Pi+1 evaluating from P(N) = Qf\n",
        "  Pip1 = eval_p_i(A, B, Q, R, Qf, N, i+1)\n",
        "\n",
        "  #get the input from the LQR controller\n",
        "  ui = eval_lqr_input(A, B, Q, R, Pip1, xi)\n",
        "\n",
        "  #apply the input to the system\n",
        "  xi = A @ xi + B @ ui\n",
        "\n",
        "  #update the history matrix\n",
        "  xHist = np.hstack((xHist, xi))\n",
        "\n",
        "#extract the results and plot\n",
        "x1Hist = xHist[0, :].tolist()  \n",
        "x2Hist = xHist[1, :].tolist()\n",
        "x3Hist = xHist[2, :].tolist()\n",
        "\n",
        "plt.plot(x1Hist)\n",
        "plt.plot(x2Hist)\n",
        "plt.plot(x3Hist)\n",
        "plt.title(\"Stabilization Under LQR Control\")\n",
        "plt.xlabel(\"Time Step\")\n",
        "plt.ylabel(\"State Vector Values\")\n",
        "plt.legend(['x1', 'x2', 'x3'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2M4lKvmEDdB"
      },
      "source": [
        "## Problem 2: Gradient Descent in Q-Learning\n",
        "In practce, Q-learning depends on an algorithm called *gradient descent* to optimally fit a Q function based on data. What is gradient descent?\n",
        "\n",
        "Gradient descent is an algorithm that helps us numerically find the minimum of a function and the argument that minimizes the function. It's widely used in machine learning, for instance, to optimize the weights in neural networks.\n",
        "\n",
        "Let's discuss the steps of the algorithm. Suppose we have a function $f(x)$ that we'd like to minimize with respect to $x$.\n",
        "1. Start with an initial guess, $x_0$, of the value of $x$ that minimizes $f(x)$.\n",
        "2. Compute the gradient of $f(x)$ with respect to $x$ at the value of the guess, $\\nabla_x f(x_0)$ (where $\\nabla_x$ denotes the gradient operator with respect to $x$).\n",
        "3. Update the guess $x_0$ through the update rule $x_1 = x_0 - \\alpha ∇_xf(x_0)$. $\\alpha$ is a value known as the *learning rate*, and controls how quickly and precisely the algorithm converges.\n",
        "4. Continue iterating the value of $x_i$ using the update rule until the algorithm converges to the minimum of $f(x)$.\n",
        "\n",
        "Why does this algorithm work? In short, we know that the direction of steepest increase of a function is given by its gradient $\\nabla_x f(x)$. If we move opposite to this direction of steepest increase, we should eventually converge to a minimum (subject to certain conditions).\n",
        "\n",
        "Let's implement the gradient descent algorithm on a very simple example. Let's find the minimum of the scalar function $f(x) = x^2$ using gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wOwyrZREGp0"
      },
      "outputs": [],
      "source": [
        "def param_update(xi, grad, alpha):\n",
        "  \"\"\"\n",
        "  Function to implement the gradient descent parameter update step\n",
        "  xi+1 <- xi - alpha * grad f(xi)\n",
        "  Inputs:\n",
        "    xi (float): guess for the minimizer x at step i\n",
        "    grad (float): gradient of the function f, calculated at xi\n",
        "    alpha (float): learning rate\n",
        "  Returns:\n",
        "    xi+1 (float): the next guess of xi based on the parameter update equation\n",
        "  \"\"\"\n",
        "  return ...\n",
        "\n",
        "def get_grad(x):\n",
        "  \"\"\"\n",
        "  Function to calculate the gradient of the function f(x) = x^2 with respect to x\n",
        "  Inputs:\n",
        "    x (float): value to evaluate the gradient at\n",
        "  Returns:\n",
        "    grad f(x) (float): gradient of f with respect to x, evaluated at x\n",
        "  \"\"\"\n",
        "  return ...\n",
        "\n",
        "def find_minimizer(x0, N, alpha):\n",
        "  \"\"\"\n",
        "  Function to perform N iterations of gradient descent to find the value of x\n",
        "  that minimizes f(x).\n",
        "  Inputs:\n",
        "    x0 (float): initial guess for the minimum of the function\n",
        "    N (int): Number of times to perform gradient descent update step\n",
        "    alpha (float): learning rate\n",
        "  Returns:\n",
        "    xN (float): estimate of the minimum after applying gradient descent N times\n",
        "  \"\"\"\n",
        "  return ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01Os8TysHC3y"
      },
      "source": [
        "#### Testing Gradient Descent\n",
        "Now that you've implemented the required functions for gradient descent, test your implementation by running the code cell below. This cell provides a visualization of gradient descent for a set of three different learning rates. You should see each plot converge to the minimum of $f(x)$ at a different rate. Attach the plot generated by the code to your solution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxQBrz1SHCEN"
      },
      "outputs": [],
      "source": [
        "#define lower and upper bounds for plotting\n",
        "lBound = -5\n",
        "uBound = -lBound\n",
        "\n",
        "#get quadratic values for plot\n",
        "numPts = 100\n",
        "f_x = lambda x: x**2\n",
        "xList = np.linspace(lBound, uBound, numPts).tolist()\n",
        "fVals = [f_x(x) for x in xList]\n",
        "\n",
        "#pick a random initial guess uniformly\n",
        "x0 = np.random.uniform(uBound-1, uBound)\n",
        "\n",
        "#define learning rate and number of iterations\n",
        "alpha1 = 0.75\n",
        "alpha2 = 0.5\n",
        "alpha3 = 0.25\n",
        "N = 10\n",
        "\n",
        "#define history arrays to keep track of iterated values\n",
        "xHist1 = []\n",
        "yHist1 = []\n",
        "xHist2 = []\n",
        "yHist2 = []\n",
        "xHist3 = []\n",
        "yHist3 = []\n",
        "\n",
        "#call gradient descent iteratively to show its progression\n",
        "for i in range(N):\n",
        "  #note: we call find_minimizer on \"i\" just so we can visualize the algorithm\n",
        "  xHist1.append(find_minimizer(x0, i, alpha1)) #append x guess to history\n",
        "  yHist1.append(f_x(xHist1[-1])) #find the y value associated with the x guess\n",
        "  \n",
        "  #repeat for second and third learning rates\n",
        "  xHist2.append(find_minimizer(x0, i, alpha2))\n",
        "  yHist2.append(f_x(xHist2[-1]))\n",
        "\n",
        "  xHist3.append(find_minimizer(x0, i, alpha3))\n",
        "  yHist3.append(f_x(xHist3[-1]))\n",
        "\n",
        "#plot results\n",
        "plt.plot(xList, fVals) #plot the function\n",
        "plt.plot(xHist1, yHist1) #plot the guess using the first learning rate\n",
        "plt.plot(xHist2, yHist2)\n",
        "plt.plot(xHist3, yHist3)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend(['f(x)', 'alpha 1', 'alpha 2', 'alpha 3'])\n",
        "plt.title(\"Convergence of Gradient Descent\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8syebT9OBek"
      },
      "source": [
        "## Problem 3: Reinforcement Learning For Legged Robotics\n",
        "In this problem, we'll use deep reinforcement learning to teach a simple legged robot system to walk!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuDF6ikogP4r"
      },
      "source": [
        "#### MuJoCo and Stable Baselines Setup\n",
        "We'll need a few libraries to train our robot to walk. First, we'll install MuJoCo. MuJoCo is a popular physics simulator in the reinforcement learning community. It contains many useful benchmark models and integrates well with standard reinforcement learning libraries.\n",
        "\n",
        "Secondly, we'll install Stable Baselines 3 and gymnasium, two environments that will enable us to actually apply reinforcement learning. Stable Baselines 3 contains a set of strong implementations of reinformcent learning algorithms. We'll use an implementation of PPO, a deep RL algorithm, from stable baselines. This library makes it exceptionally easy to apply RL in practice! Gymnasium will provide us with an environment that interfaces between our deep RL policy and our MuJoCo model.\n",
        "\n",
        "**Before running the cells below, click the \"runtime\" tab at the top of the page and then \"change runtime type.\" Ensure \"GPU\" is selected under this option. This is required by MuJoCo to run correctly.** Once you've done this, run the cells below to install these libraries! Note that the code cells may take around a minute to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWI_7hvyhObD"
      },
      "outputs": [],
      "source": [
        "!pip install mujoco==2.3.3\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import distutils.util\n",
        "import subprocess\n",
        "if subprocess.run('nvidia-smi').returncode:\n",
        "  raise RuntimeError(\n",
        "      'Cannot communicate with GPU. '\n",
        "      'Make sure you are using a GPU Colab runtime. '\n",
        "      'Go to the Runtime menu and select Choose runtime type.')\n",
        "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
        "print('Setting environment variable to use GPU rendering:')\n",
        "%env MUJOCO_GL=egl\n",
        "\n",
        "try:\n",
        "  print('Checking that the installation succeeded:')\n",
        "  import mujoco\n",
        "  mujoco.MjModel.from_xml_string('<mujoco/>')\n",
        "except Exception as e:\n",
        "  raise e from RuntimeError(\n",
        "      'Something went wrong during installation. Check the shell output above '\n",
        "      'for more information.\\n'\n",
        "      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n",
        "      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n",
        "\n",
        "print('Installation successful.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laSiAL-nY0vv"
      },
      "outputs": [],
      "source": [
        "!apt-get update && apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\"\n",
        "!pip install glfw\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD9XvS69-10v"
      },
      "source": [
        "#### Defining the Training Environment\n",
        "When setting up a reinforcement learning problem, the first thing we must do is define the training environment. We may do this with the library *gymnasium*. Gymnasium, or ``gym\" environments are a standard type of environment in reinforcement learning.\n",
        "\n",
        "To create an environment, we first import the gymnasium library. Then, to make an environment, we use the syntax:\n",
        "```\n",
        "import gymnasium as gym\n",
        "env = gym.make('env-name')\n",
        "```\n",
        "Where 'env-name' is the name of the environment we wish to use. The gym library has a set of built-in environments we can use, but also supports custom environments.\n",
        "\n",
        "Fill in the cell below by creating an environment with the name **'HalfCheetah-v4'**. This will create our legged robotics environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_1xS2oW_xVP"
      },
      "outputs": [],
      "source": [
        "#import gym\n",
        "import gymnasium as gym\n",
        "\n",
        "#make the environment 'HalfCheetah-v4'\n",
        "env = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmpF0vem_73R"
      },
      "source": [
        "#### Defining the Policy\n",
        "Now that we've defined our environment, we can define our reinforcement learning *algorithm*. This will determine the numerical procedure our system will use to determine an optimal control policy for our robot.\n",
        "\n",
        "The library *Stable Baselines 3* has a set of implementations of popular reinforcement learning libraries that integrate smoothly with Gym environments. We'll use an algorithm known as *Proximal Policy Optimization* (PPO). This is a popular RL algorithm that'll help us converge efficiently to an optimal policy. Stable baselines also has implementations of other popular algorithms such as deep Q-learning (DQN) and advantage actor critic (A2C).\n",
        "\n",
        "To define a PPO algorithm for our environment, we use the following syntax:\n",
        "```\n",
        "model = PPO('policyType', env, verbose = 1)\n",
        "```\n",
        "Where 'policyType' is a string that refers to the type of neural network used in the algorithm, 'env' is our environment from above, and verbose specifies some printing options during the training procedure.\n",
        "\n",
        "In the code cell below, create an instance of a PPO algorithm called policyModel. Your PPO should use a policy type called 'MlpPolicy' - this refers to a standard type of neural network called a multi-layer perceptron. You should pass in env from above and set verbose = 1.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GU9kNdGB7dA"
      },
      "outputs": [],
      "source": [
        "#import some algorithms from stable baselines 3\n",
        "from stable_baselines3 import SAC, PPO, DQN, A2C\n",
        "\n",
        "#declare an initial observation (don't edit this line)\n",
        "obs = env.reset()\n",
        "\n",
        "#define your PPO algorithm using an 'MlpPolicy' - remember to pass this in as a string.\n",
        "policyModel = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nojpsTxICopT"
      },
      "source": [
        "#### Training Your RL System\n",
        "Now that we've defined our environment and algorithm, we're ready to begin the training process! This is the process by which we simulate our system, collect data, and iteratively improve our control policy.\n",
        "\n",
        "To train our policy over N iterations, we use the syntax:\n",
        "```\n",
        "model.learn(total_timesteps = N)\n",
        "```\n",
        "Where model is the name of the algorithm instance we created in the previous step and N is some positive integer. Generally, the higher N is, and the more iterations we have, the better our policy will be! The basic idea behind this is that for larger N, we'll be able to collect more data and therefore get a better idea of how to control our system.\n",
        "\n",
        "In the code cell below, call the .learn() function on the policyModel from the previous step. Set the value of N to be somewhere in the region of 700,000 - you should be able to get reasonable performance from the legged robot in this scenario for this value. The training process will take around 20-30 minutes to complete with this number of iterations.\n",
        "\n",
        "As the training proceeds, a window will be printed specifying the current performance of the system. To gain an idea of how your system is performing, look at the data point titled 'ep_rew_mean' - this tells you the mean reward obtained by the system for a training episode - the higher this value is, the better! You should see this number increase greatly as the training proceeds.\n",
        "\n",
        "With this in mind, in the code cell below, call the .learn() function on the policyModel from the previous step for N ~ 700,000 to train the model. Remember, this will take some time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abunmk8zE9gf"
      },
      "outputs": [],
      "source": [
        "#call .learn(total_timesteps = N) on policyModel to train the system\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uedWnv_ihHgV"
      },
      "source": [
        "#### Testing the Learned Control Policy\n",
        "Now that we've finished training the model, we can test it out on the system and observe its performance! After we've trained the model, we retrieve a copy of the environment. We may use the syntax:\n",
        "```\n",
        "vec_env = policyModel.get_env()\n",
        "```\n",
        "After this, we can test out how our control policy performs by \"stepping\" the environment forward in time through our control policy. We can examine the behavior of our system under 1000 steps, for instance, using the syntax:\n",
        "```\n",
        "obs = vec_env.reset()\n",
        "for i in range(1000):\n",
        "    action, _state = policyModel.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = vec_env.step(action)\n",
        "```\n",
        "Where policyModel.predict() returns the optimal action of our system given an observation of its current state.\n",
        "\n",
        "Run the following code cell, which performs this simulation procedure and generates a video of the robot. Note that the extra functions in this code cell are simply required by nature of the Colab environment. In a standard Python file, the code above would be all we'd need to test our system.\n",
        "\n",
        "If your training process was successful, you should see the half-cheetah robot moving to the right! It's alright if it performs the task in a way you weren't expecting - this is part of the challenge of RL!\n",
        "\n",
        "As this cell renders a video, it will take around a minute to watch. Record the video and attach a link to your recording (either a YouTube or Google Drive link) to your solution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cCW1tl9ydMXe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import base64\n",
        "from pathlib import Path\n",
        "from IPython import display as ipythondisplay\n",
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "# Set up display\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "def show_videos(video_path=\"\", prefix=\"\"):\n",
        "    \"\"\"\n",
        "    :param video_path: (str) Path to the folder containing videos\n",
        "    :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "    \"\"\"\n",
        "    html = []\n",
        "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append(\n",
        "            \"\"\"<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>\"\"\".format(\n",
        "                mp4, video_b64.decode(\"ascii\")\n",
        "            )\n",
        "        )\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix=\"\", video_folder=\"videos/\"):\n",
        "    \"\"\"\n",
        "    :param env_id: (str)\n",
        "    :param model: (RL model)\n",
        "    :param video_length: (int)\n",
        "    :param prefix: (str)\n",
        "    :param video_folder: (str)\n",
        "    \"\"\"\n",
        "    eval_env = DummyVecEnv([lambda: gym.make(\"HalfCheetah-v4\", render_mode=\"rgb_array\")])\n",
        "    # Start the video at step=0 and record 500 steps\n",
        "    eval_env = VecVideoRecorder(\n",
        "        eval_env,\n",
        "        video_folder=video_folder,\n",
        "        record_video_trigger=lambda step: step == 0,\n",
        "        video_length=video_length,\n",
        "        name_prefix=prefix,\n",
        "    )\n",
        "\n",
        "    obs = eval_env.reset()\n",
        "    for _ in range(video_length):\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "    # Close the video recorder\n",
        "    eval_env.close()\n",
        "  \n",
        "#Run video recording!\n",
        "record_video(\"HalfCheetah-v4\", policyModel, video_length=500, prefix=\"policy-cheetah\")\n",
        "show_videos(\"videos\", prefix=\"policy\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}